{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geração de Embeddings com Google Gemini\n",
    "\n",
    "**Objetivo:** Gerar embeddings de textos usando o modelo Google Gemini via API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ClassicVsModernNLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar variáveis de ambiente (opcional, se usar .env)\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configurar chave da API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API configurada com sucesso!\n",
      "Modelo selecionado: models/embedding-001\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# --- Definir chave diretamente no código (apenas para testes locais) ---\n",
    "api_key = \"AIzaSyBB_U3VHjmGq3G-j4EblDI32eU-mcO7ebA\"\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Configurar o cliente Gemini\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Definir modelo de embeddings (ou qualquer outro modelo)\n",
    "model = \"models/embedding-001\"\n",
    "\n",
    "print(\"API configurada com sucesso!\")\n",
    "print(f\"Modelo selecionado: {model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregar dataset pré-processado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de documentos: 5611\n",
      "Classes: ['rec.autos', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.space', 'talk.politics.guns', 'talk.politics.mideast']\n"
     ]
    }
   ],
   "source": [
    "# Carregar dados pré-processados\n",
    "with open('../data/processed/20news_preprocessed.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_text = data['text']\n",
    "y = data['target']\n",
    "target_names = data['target_names']\n",
    "\n",
    "print(f\"Total de documentos: {len(X_text)}\")\n",
    "print(f\"Classes: {target_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Função para gerar embeddings em lotes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Alternativa: Embeddings Locais (se API não disponível)\n",
    "\n",
    "Se a API do Gemini não estiver disponível (quota excedida), você pode usar `sentence-transformers` para gerar embeddings localmente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função alternativa usando sentence-transformers (não requer API)\n",
    "def generate_embeddings_local(texts, model_name=\"all-MiniLM-L6-v2\", batch_size=32, use_tqdm=True):\n",
    "    \"\"\"\n",
    "    Gera embeddings usando sentence-transformers localmente.\n",
    "    Não requer API, funciona offline após instalar a biblioteca.\n",
    "    \n",
    "    Para instalar: pip install sentence-transformers\n",
    "    \n",
    "    Args:\n",
    "        texts: Lista de textos\n",
    "        model_name: Nome do modelo sentence-transformers (padrão: all-MiniLM-L6-v2)\n",
    "        batch_size: Tamanho do lote para processamento\n",
    "        use_tqdm: Se True, usa barra de progresso\n",
    "    \n",
    "    Returns:\n",
    "        Array numpy com embeddings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"sentence-transformers não está instalado!\\n\"\n",
    "            \"Instale com: pip install sentence-transformers\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Carregando modelo {model_name}...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    print(f\"Gerando embeddings para {len(texts)} textos...\")\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=use_tqdm,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Embeddings gerados! Shape: {embeddings.shape}\")\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "\n",
    "def generate_embeddings_batch(texts, model_name, batch_size=1, delay=2.0, use_tqdm=True):\n",
    "    \"\"\"\n",
    "    Gera embeddings com controle rigoroso de rate limiting.\n",
    "    \n",
    "    IMPORTANTE: A API gratuita do Gemini tem limite de 0 para embeddings!\n",
    "    Se receber erro 429, você precisa:\n",
    "    1. Aguardar 24h para reset da quota diária, OU\n",
    "    2. Usar um plano pago, OU\n",
    "    3. Processar texto por texto com delay maior (2+ segundos)\n",
    "    \n",
    "    Args:\n",
    "        texts: Lista de textos\n",
    "        model_name: Nome do modelo (ex: \"models/embedding-001\")\n",
    "        batch_size: Tamanho do lote (recomendado: 1 para free tier)\n",
    "        delay: Delay em segundos entre requisições (recomendado: 2.0+ para free tier)\n",
    "        use_tqdm: Se True, usa barra de progresso tqdm\n",
    "    \n",
    "    Returns:\n",
    "        Array numpy com embeddings\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    n_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"Gerando embeddings para {len(texts)} textos em {n_batches} lotes (batch_size={batch_size}, delay={delay}s)...\")\n",
    "    print(\"AVISO: Free tier tem limites restritivos. Processando lentamente...\")\n",
    "    \n",
    "    # Criar barra de progresso\n",
    "    if use_tqdm:\n",
    "        pbar = tqdm(total=len(texts), desc=\"Gerando embeddings\", unit=\"text\")\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_num = i // batch_size + 1\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        success = False\n",
    "        \n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                # Gerar embeddings para o lote\n",
    "                result = genai.embed_content(\n",
    "                    model=model_name,\n",
    "                    content=batch,\n",
    "                    task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "                )\n",
    "                \n",
    "                # Extrair embeddings - a API pode retornar de diferentes formas\n",
    "                if isinstance(result, dict):\n",
    "                    if 'embedding' in result:\n",
    "                        batch_embeddings = result['embedding']\n",
    "                        if isinstance(batch_embeddings, list):\n",
    "                            # Se for lista de listas (um embedding por texto)\n",
    "                            if len(batch_embeddings) > 0 and isinstance(batch_embeddings[0], list):\n",
    "                                embeddings.extend(batch_embeddings)\n",
    "                            else:\n",
    "                                # Se for um único embedding\n",
    "                                embeddings.extend([batch_embeddings])\n",
    "                        else:\n",
    "                            embeddings.append(batch_embeddings)\n",
    "                    else:\n",
    "                        # Tentar primeira chave\n",
    "                        batch_embeddings = list(result.values())[0] if result else []\n",
    "                        if isinstance(batch_embeddings, list):\n",
    "                            embeddings.extend(batch_embeddings if isinstance(batch_embeddings[0], list) else [batch_embeddings])\n",
    "                elif isinstance(result, list):\n",
    "                    embeddings.extend(result)\n",
    "                else:\n",
    "                    embeddings.append(result)\n",
    "                \n",
    "                success = True\n",
    "                if use_tqdm:\n",
    "                    pbar.update(len(batch))\n",
    "                else:\n",
    "                    if batch_num % 10 == 0 or batch_num == n_batches:\n",
    "                        print(f\"Lote {batch_num}/{n_batches} concluído ({len(batch)} embedding(s))\")\n",
    "                \n",
    "            except google_exceptions.ResourceExhausted as e:\n",
    "                # Erro 429 - Quota excedida\n",
    "                error_msg = str(e)\n",
    "                if \"free_tier\" in error_msg.lower() or \"limit: 0\" in error_msg:\n",
    "                    if use_tqdm:\n",
    "                        pbar.close()\n",
    "                    print(f\"\\n{'='*60}\")\n",
    "                    print(\"ERRO: Quota da API gratuita excedida!\")\n",
    "                    print(\"Soluções:\")\n",
    "                    print(\"1. Aguardar 24h para reset da quota diária\")\n",
    "                    print(\"2. Atualizar para plano pago no Google Cloud\")\n",
    "                    print(\"3. Verificar quota em: https://ai.dev/usage?tab=rate-limit\")\n",
    "                    print(f\"{'='*60}\")\n",
    "                    raise Exception(\"Quota da API gratuita excedida. Consulte https://ai.google.dev/gemini-api/docs/rate-limits\")\n",
    "                \n",
    "                # Rate limit temporário - aguardar e tentar novamente\n",
    "                wait_time = delay * (2 ** retry_count)  # Backoff exponencial\n",
    "                if use_tqdm:\n",
    "                    pbar.set_description(f\"Rate limit - aguardando {wait_time:.1f}s...\")\n",
    "                else:\n",
    "                    print(f\"Rate limit no lote {batch_num}. Aguardando {wait_time:.1f}s antes de retry {retry_count+1}/{max_retries}...\")\n",
    "                \n",
    "                time.sleep(wait_time)\n",
    "                retry_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                if \"429\" in error_msg or \"quota\" in error_msg.lower() or \"ResourceExhausted\" in error_msg:\n",
    "                    wait_time = delay * (2 ** retry_count)\n",
    "                    if use_tqdm:\n",
    "                        pbar.set_description(f\"Erro 429 - aguardando {wait_time:.1f}s...\")\n",
    "                    else:\n",
    "                        print(f\"Erro 429 no lote {batch_num}. Aguardando {wait_time:.1f}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    retry_count += 1\n",
    "                else:\n",
    "                    # Outro tipo de erro - não tentar novamente\n",
    "                    if use_tqdm:\n",
    "                        pbar.set_description(f\"Erro no lote {batch_num}\")\n",
    "                    print(f\"\\nErro no lote {batch_num}: {error_msg[:200]}\")\n",
    "                    # Gerar embedding vazio para manter consistência\n",
    "                    embedding_dim = 768 if len(embeddings) == 0 else len(embeddings[0])\n",
    "                    embeddings.extend([[0.0] * embedding_dim] * len(batch))\n",
    "                    if use_tqdm:\n",
    "                        pbar.update(len(batch))\n",
    "                    success = True  # Marcar como sucesso para não retry infinito\n",
    "        \n",
    "        if not success:\n",
    "            # Após todas as tentativas falharem\n",
    "            if use_tqdm:\n",
    "                pbar.set_description(f\"Erro persistente no lote {batch_num}\")\n",
    "            print(f\"\\nErro persistente no lote {batch_num} após {max_retries} tentativas\")\n",
    "            embedding_dim = 768 if len(embeddings) == 0 else len(embeddings[0])\n",
    "            embeddings.extend([[0.0] * embedding_dim] * len(batch))\n",
    "            if use_tqdm:\n",
    "                pbar.update(len(batch))\n",
    "        \n",
    "        # Delay entre lotes para evitar rate limiting\n",
    "        if i + batch_size < len(texts):\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    if use_tqdm:\n",
    "        pbar.close()\n",
    "    \n",
    "    return np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gerar embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GERANDO EMBEDDINGS COM SENTENCE-TRANSFORMERS (LOCAL)\n",
      "============================================================\n",
      "Vantagens:\n",
      "  - Não requer API ou quota\n",
      "  - Funciona offline\n",
      "  - Muito mais rápido\n",
      "  - Grátis e ilimitado\n",
      "============================================================\n",
      "\n",
      "Carregando modelo all-MiniLM-L6-v2...\n",
      "Gerando embeddings para 5611 textos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 176/176 [00:48<00:00,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings gerados! Shape: (5611, 384)\n",
      "\n",
      "============================================================\n",
      "Embeddings gerados com SENTENCE-TRANSFORMERS!\n",
      "Shape: (5611, 384)\n",
      "Dimensão do embedding: 384\n",
      "Modelo usado: sentence-transformers/all-MiniLM-L6-v2\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Converter para lista de strings (se necessário)\n",
    "texts_list = [str(text) for text in X_text]\n",
    "\n",
    "# ============================================================================\n",
    "# OPÇÃO 1: Tentar usar Gemini API (requer quota disponível)\n",
    "# ============================================================================\n",
    "USE_GEMINI = False  # Altere para True se tiver quota disponível na API\n",
    "\n",
    "if USE_GEMINI:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TENTANDO GERAR EMBEDDINGS COM GEMINI API\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total de textos: {len(texts_list)}\")\n",
    "    print(f\"Batch size: 1 (texto por texto)\")\n",
    "    print(f\"Delay entre requisições: 2.0 segundos\")\n",
    "    print(f\"Tempo estimado: ~{(len(texts_list) * 2.0) / 60:.1f} minutos (~{(len(texts_list) * 2.0) / 3600:.1f} horas)\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        X_emb = generate_embeddings_batch(\n",
    "            texts_list, \n",
    "            model, \n",
    "            batch_size=1,\n",
    "            delay=2.0,\n",
    "            use_tqdm=True\n",
    "        )\n",
    "        model_name_used = model\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Embeddings gerados com GEMINI API!\")\n",
    "        print(f\"Shape: {X_emb.shape}\")\n",
    "        print(f\"Dimensão do embedding: {X_emb.shape[1]}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ERRO ao usar Gemini API:\")\n",
    "        print(str(e)[:200])\n",
    "        print(\"\\nAlternando para embeddings locais (sentence-transformers)...\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        USE_GEMINI = True\n",
    "\n",
    "# ============================================================================\n",
    "# OPÇÃO 2: Usar sentence-transformers (alternativa local, recomendada)\n",
    "# ============================================================================\n",
    "if not USE_GEMINI:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"GERANDO EMBEDDINGS COM SENTENCE-TRANSFORMERS (LOCAL)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Vantagens:\")\n",
    "    print(\"  - Não requer API ou quota\")\n",
    "    print(\"  - Funciona offline\")\n",
    "    print(\"  - Muito mais rápido\")\n",
    "    print(\"  - Grátis e ilimitado\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Modelo recomendado: all-MiniLM-L6-v2 (rápido e eficiente)\n",
    "    # Outros modelos disponíveis: all-mpnet-base-v2, paraphrase-multilingual-MiniLM-L12-v2\n",
    "    local_model_name = \"all-MiniLM-L6-v2\"\n",
    "    \n",
    "    X_emb = generate_embeddings_local(\n",
    "        texts_list,\n",
    "        model_name=local_model_name,\n",
    "        batch_size=32,  # Pode ser maior para modelos locais\n",
    "        use_tqdm=True\n",
    "    )\n",
    "    model_name_used = f\"sentence-transformers/{local_model_name}\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Embeddings gerados com SENTENCE-TRANSFORMERS!\")\n",
    "    print(f\"Shape: {X_emb.shape}\")\n",
    "    print(f\"Dimensão do embedding: {X_emb.shape[1]}\")\n",
    "    print(f\"Modelo usado: {model_name_used}\")\n",
    "    print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Salvar embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Embeddings salvos com sucesso!\n",
      "Arquivo: ../data/processed/embeddings_gemini.pkl\n",
      "Total de embeddings: 5611\n",
      "Dimensão: 384\n",
      "Modelo usado: sentence-transformers/all-MiniLM-L6-v2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Criar diretório se não existir\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Preparar dados para salvar\n",
    "data_to_save = {\n",
    "    'X_emb': X_emb,\n",
    "    'y': y,\n",
    "    'target_names': target_names,\n",
    "    'model_name': model_name_used  # Usar o modelo que foi efetivamente usado\n",
    "}\n",
    "\n",
    "# Salvar em pickle (nome do arquivo mantém \"gemini\" para compatibilidade, mas pode conter embeddings locais)\n",
    "output_path = '../data/processed/embeddings_gemini.pkl'\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(data_to_save, f)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Embeddings salvos com sucesso!\")\n",
    "print(f\"Arquivo: {output_path}\")\n",
    "print(f\"Total de embeddings: {len(X_emb)}\")\n",
    "print(f\"Dimensão: {X_emb.shape[1]}\")\n",
    "print(f\"Modelo usado: {model_name_used}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ClassicVsModernNLP)",
   "language": "python",
   "name": "classicvsmodernnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
